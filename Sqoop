官方文档
http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_introduction

Sqoop
https://www.cnblogs.com/qingyunzong/p/8807252.html
sqoop to hive
https://www.cnblogs.com/xuyou551/p/7998846.html

Sqoop常用命令

## sqoop --options-file  ./sqoop_test1.txt

#import to HDFS
import
--connect 
jdbc:mysql://172.16.71.27:3306/babasport 
--username 
root
--password 
root
--query 
'select id, brand_id,name from bbs_product where $CONDITIONS LIMIT 100'
--target-dir 
/user/xuyou/sqoop/imp_bbs_product_sannpy_
--delete-target-dir
--num-mappers
1
--compress
--compression-codec 
org.apache.hadoop.io.compress.SnappyCodec
--fields-terminated-by 
'\t'`
--where "columnname='XXXX'"
1.如果num-mappers大于1，怎么需要设定分区
2.
在以上需要按照自定义SQL语句导出数据到HDFS的情况下：
2.1、引号问题，要么外层使用单引号，内层使用双引号，$CONDITIONS的$符号不用转义， 要么外层使用双引号，那么内层使用单引号，然后$CONDITIONS的$符号需要转义
2.2、自定义的SQL语句中必须带有WHERE \$CONDITIONS

#Import to hive
sqoop import  \
--connect jdbc:mysql://hadoop1:3306/mysql  \
--username root  \
--password root  \
--table help_keyword  \
--fields-terminated-by "\t"  \
--lines-terminated-by "\n"  \
--hive-import  \
--hive-overwrite  \
--create-hive-table  \
--delete-target-dir \
--hive-database  mydb_test \
--hive-table new_help_keyword
--incremental  append  \
--check-column  help_keyword_id \
--last-value 500  \

并行机制：
通过这只--num-mappers来设置并行导入的mapreduce的数量，如果不设置默认是4个。 
一但设置了并行导入，那么就需要告诉sqoop（底层的mapreduce）用什么来切片，如果输入的表，那一般使用表的主键切片。所以还需要添加 --split by，否则会报错。
对于输入端是数据库的，mappper task的数量必须手动设置，特别是表比较大的时候，否则使用默认的4个map，会造成单个map负责过大，导入效率低的问题。
那多少个mapper task合适？ 一般用表大小/块大小，就等于maptask的数量。 比如5000M/128M。

